{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faee480d-f504-4051-b7c7-71646d521873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Introduction Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "190e6586-646f-411d-8765-84702d46a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Importing Libraries\n",
    "import pickle\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15f7a4d-1b94-44ea-bfc9-8b0b9d9788f1",
   "metadata": {},
   "source": [
    "### The dataset we will be using is the NEWS datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d71b1c2-4c34-4657-b854-3835d13afe36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SulabhShrestha\\AppData\\Local\\Temp\\ipykernel_6524\\821131068.py:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(\"abcnews-date-text.csv\",error_bad_lines=False,usecols =[\"headline_text\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  aba decides against community broadcasting lic...\n",
       "1     act fire witnesses must be aware of defamation\n",
       "2     a g calls for infrastructure protection summit\n",
       "3           air nz staff in aust strike for pay rise\n",
       "4      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"abcnews-date-text.csv\",error_bad_lines=False,usecols =[\"headline_text\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a70bb3-5743-4219-82cc-5296d69ea875",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05caf289-137b-40a6-a050-8a4839bd2d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57973</th>\n",
       "      <td>10 killed in pakistan bus crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116304</th>\n",
       "      <td>10 killed in pakistan bus crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912357</th>\n",
       "      <td>110 with barry nicholls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673104</th>\n",
       "      <td>110 with barry nicholls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676569</th>\n",
       "      <td>110 with barry nicholls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748865</th>\n",
       "      <td>110 with barry nicholls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827317</th>\n",
       "      <td>110 with barry nicholls episode 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898182</th>\n",
       "      <td>110 with barry nicholls episode 15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             headline_text\n",
       "57973      10 killed in pakistan bus crash\n",
       "116304     10 killed in pakistan bus crash\n",
       "912357             110 with barry nicholls\n",
       "673104             110 with barry nicholls\n",
       "676569             110 with barry nicholls\n",
       "748865             110 with barry nicholls\n",
       "827317  110 with barry nicholls episode 15\n",
       "898182  110 with barry nicholls episode 15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['headline_text'].duplicated(keep=False)].sort_values('headline_text').head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ef2bc79-a3a2-4ca7-b413-8f6e1da71632",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates('headline_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0643e7d5-d124-41eb-9e0a-b29776d35727",
   "metadata": {},
   "source": [
    "However, when doing natural language processing, words must be converted into vectors that machine learning algorithms can make use of. If your goal is to do machine learning on text data, like movie reviews or tweets or anything else, you need to convert the text data into numbers. This process is sometimes referred to as “embedding” or “vectorization”.\n",
    "\n",
    "In terms of vectorization, it is important to remember that it isn’t merely turning a single word into a single number. While words can be transformed into numbers, an entire document can be translated into a vector. Not only can a vector have more than one dimension, but with text data vectors are usually high-dimensional. This is because each dimension of your feature data will correspond to a word, and the language in the documents you are examining will have thousands of words.\n",
    "\n",
    "TF-IDF\n",
    "In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. Nowadays, tf-idf is one of the most popular term-weighting schemes; 83% of text-based recommender systems in the domain of digital libraries use tf-idf.\n",
    "\n",
    "Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.\n",
    "\n",
    "One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a97382b-fa33-4841-ae13-6eac19d8a39e",
   "metadata": {},
   "source": [
    "### Natural Language Processing\n",
    "<p> Just like last time, where we encoded the datetime to fit the machine learning model, we also need to change the datasets to fit the Cluster algorithm </p>\n",
    "<p> In NLP, \n",
    "    <ul>\n",
    "        <li> we remove stopwords like '.', ',' \n",
    "            <li>we get the root word of data from the process of stemming. Example: go, went and gone all has the common root word of move</li>\n",
    "    <li>we break the sentence into words and punctuation called Tokenization</li>\n",
    "        <li> we convert the datasets into vectors (embedding/vectorization)</li>\n",
    "        <li> we use Term Frequency-Inverse Document Frequency to reflect the importance of a word to the document </li>   \n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cd37c68-bb23-4b8e-8242-5a78e5ec1edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SulabhShrestha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# # Removing Stop Words\n",
    "punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(punc)\n",
    "desc = data['headline_text'].values\n",
    "\n",
    "# # Use Stemming and Tokenization\n",
    "stemmer = SnowballStemmer('english')\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n",
    "\n",
    "def tokenize(text):\n",
    "    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]\n",
    "\n",
    "# # Vectorize using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = 1000)\n",
    "X = vectorizer.fit_transform(desc)\n",
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc9292a-60f5-4eaf-92f5-22163373b6ee",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "-- description for unsuperbised learning with references\n",
    "\n",
    "### K-Means Clustering\n",
    "--description fo K-Means Clustering with references"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f3b722-8426-4cd0-a0bc-dd8f04e558d6",
   "metadata": {},
   "source": [
    "Due to the limitations of the PC computer power, we were not able to train the model on the local computer. For the solution we rented a Virtual Machine on the Azure and use its compute power to run the Clustering algorithm for us, create the model, save it as a pickle file and use that pickle file here on the notebook. First we save the X value as a pickle file and load it onto the VM for it to train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d04cc60-de1c-4274-b2ee-46fd4fd036d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pickling X for the CLustering algorithm to train on\n",
    "with open(\"X.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(X, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82c78a59-072e-4ade-b546-7930e470025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pickling words for the CLustering algorithm to train on\n",
    "with open(\"words.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(words, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f538801e-4820-4d94-96fc-3bdc62f57420",
   "metadata": {},
   "source": [
    "So, the train function will run on the Azure Virtual Machine and will give us the model file as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ec007fb-6bd8-47cf-803c-af971861527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training the model\n",
    "\n",
    "# # Unpickling\n",
    "# with open(\"X.pkl\", \"rb\") as fp:\n",
    "#     X = pickle.load(fp)\n",
    "# number_of_cluster = 10\n",
    "    \n",
    "def train(number_of_cluster, X):\n",
    "    kmeans = KMeans(n_clusters = number_of_cluster, n_init = 20)\n",
    "    kmeans.fit(X)\n",
    "    \n",
    "    pickle.dump(kmeans, open('model.pkl', 'wb'))\n",
    "    print('Model is pickled and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a691e97-dc93-49ca-b72e-c48ed06447b4",
   "metadata": {},
   "source": [
    "Proof of Training on Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a423aae1-fa8a-4625-9c68-d4095b5a868d",
   "metadata": {},
   "source": [
    "<img src=\"1.png\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e85cdd-73a9-445a-a77c-437b96d537c1",
   "metadata": {},
   "source": [
    "Training Process on Azure VM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a4aa65-df93-4d46-90a8-01c93e45f062",
   "metadata": {},
   "source": [
    "<img src=\"2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a988b36e-f6bd-4580-b76c-7d993b8b68ac",
   "metadata": {},
   "source": [
    "Now we load the model on the local and see the test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bbfc9ea-424d-4b36-8b4a-da158acf4526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : say, council, govt, australia, court, report, warn, fund, urg, water, face, accus, man, health, chang, boost, nsw, consid, qld, public, cut, ban, wa, hear, rural\n",
      "1 : man, death, nsw, sydney, year, open, hit, jail, attack, claim, wa, set, miss, chang, qld, hous, day, world, home, die, hospit, elect, talk, final, return\n",
      "2 : polic, interview, investig, probe, man, search, hunt, offic, miss, arrest, death, car, shoot, drug, seek, attack, say, driver, crash, murder, fatal, assault, suspect, extend, warn\n",
      "3 : charg, man, murder, face, assault, drug, polic, child, sex, woman, court, teen, death, stab, drop, alleg, rape, men, attack, guilti, shoot, bail, sydney, fatal, yo\n",
      "4 : new, zealand, law, year, open, council, polic, home, deal, hospit, centr, set, hope, australia, appoint, look, announc, chief, say, minist, govt, mayor, south, rule, servic\n",
      "5 : support, communiti, indigen, group, urg, council, servic, fund, govt, remot, say, aborigin, ralli, offer, seek, centr, health, nt, need, labor, chang, mp, govern, school, help\n",
      "6 : kill, crash, car, man, plane, die, fatal, bomb, driver, blast, injur, road, attack, iraq, woman, soldier, bus, truck, dead, afghan, highway, accid, pakistan, train, suicid\n",
      "7 : win, award, cup, titl, gold, open, stage, world, final, tour, elect, australia, lead, aussi, seri, claim, second, big, grand, england, m, australian, battl, record, race\n",
      "8 : plan, council, govt, new, water, say, develop, hous, group, chang, unveil, reject, park, urg, centr, public, expans, green, resid, health, reveal, govern, labor, opposit, power\n",
      "9 : australian, worker, open, strike, share, pay, dollar, union, market, job, year, south, say, help, health, aid, industri, day, face, death, sack, urg, foreign, die, fall\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = pickle.load(open(\"model.pkl\", \"rb\"))\n",
    "\n",
    "# use pickled loaded model to predict\n",
    "common_words = model.cluster_centers_.argsort()[:,-1:-26:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7924c0d4-b843-44c4-8f09-2c19e0935d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
